\chapter{\label{ML}Machine Learning}
% \epigraph{Machine Learning can be defined as the process of inducing intelligence into a system or machine without explicit programming}{-Andrew NG}

Machine learning (ML) is a common term used to describe all automated reasoning techniques in the broadest sense. Therefore, machine learning plays an important role in almost every area of high energy physics. Traditionally, machine learning has been synonymous with "multivariate technology."  Boosted decision trees as a community favorite method and TMVA  as a community favorite tool. The set of methods and tools commonly used in HEP has grown significantly in recent years\cite{https://doi.org/10.48550/arxiv.2102.02770}. \\
Machine learning techniques are designed to leverage large datasets  to reduce complexity and discover new features in the data. The  most commonly used machine learning algorithms in HEP today are Boosted Decision Trees (BDT) and Neural Networks (NN). Variables related to physical problems are typically selected and  machine learning models are trained  using signal and background events for classification or regression (for example). Model training is the most human and CPU-time-consuming step, but the application called the  inference phase is relatively inexpensive. BDT and NN are typically used to classify particles and events. These are also used for regression. In regression, for example, a continuous function is trained to get the best estimate of the energy of a particle based on. 
 Measurements from several detectors.Neural networks have been used in HEP ​​for some time; however, improvements in training algorithms and computing power have led to the so-called deep learning revolution over the last decade, which has had a major impact on HEP\cite{https://doi.org/10.48550/arxiv.2102.02770}. Deep learning is especially promising when there is a large amount of data and functionality, and there are symmetries and complex nonlinear dependencies between inputs and outputs. There are different types of DNNs used in HEP: Full Connection (FCN), Convolution (CNN), and Recurrent (RNN). In addition, neural networks are used in the context of generative models, and neural networks are trained to reproduce the multidimensional distribution of a set of training instances. Variational Autoencoders (VAEs) and the new Generative Adversarial Network (GAN) are two examples of such generative models used in HEP.\\

Numerous machine learning algorithms are dedicated to time series analysis and prediction. If the events are independent of each other, they are usually unrelated to HEP data analysis\cite{https://doi.org/10.48550/arxiv.2008.00444,Ismail_Fawaz_2019}. However, there is growing interest in these algorithms for data quality monitoring, computing and accelerator infrastructure, physical processes, and event reconstruction tasks where time is an important factor\cite{Alzubaidi2021}. 
 This chapter describes the basic concepts of machine learning and how neural networks work, which is more efficient and productive than traditional workflows.



\section{Types of Machine Learning:}
There are four basic approaches for machine learning: supervised learning, unsupervised learning, semi-supervised learning, and reinforcement learning\cite{Sarker2021}. The type of algorithm that will be used depends on what type of data they want to predict.

\subsection{Supervised Learning}
In this type of machine learning, we supply algorithms with labeled training datasets and define the variables that we want the algorithm to assess for correlations and the outputs. 
Both the input and the output of the algorithm have been already specified. Supervised machine learning requires getting trained with the algorithm on both labeled inputs and desired outputs. A simple example would be the classification of datasets\cite{https://doi.org/10.48550/arxiv.2003.05199}.

\subsection{Unsupervised learning:}
This type of machine learning involves algorithms that train on unlabeled datasets. The algorithm scans through datasets looking for any meaningful connection without any human intervention. The data on which algorithms train as well as the predict the output is predetermined. For example, unsupervised learning is used for Google news categorization, visual perception tasks such as image recognition, anomaly detection, classifying customers, etc\cite{Ghahramani2004}.

\subsection{Semi-supervised learning:}
This approach to machine learning involves a mix of both the previous types of the dataset; that is algorithm is trained upon the combination of the labeled and unlabeled dataset.  In this type of algorithm, the programmer just needs to cluster similar data using an unsupervised learning algorithm and further use the existing labeled data to label the rest of the unlabeled data. A few practical examples of this type of learning are speech analysis, internet content classifications, and protein sequence classification\cite{Shental05semi-supervisedlearning}.


\subsection{Reinforcement learning: }
Data scientists usually use reinforcement learning to teach a machine to complete a multistep process( based on the rewarding behaviors/ or pushing the undesired one ) for which there is a clearly defined set of rules. In the reinforcement learning decision is dependent on the output of the previous input sequence, so we provide labels to sequences of dependent decisions. For example, chess games or a sitting cat, the cat will only get food when she starts to walk\cite{Thrun92efficientexploration}.\\

A brief summary of how machine learning consists can be summarized from this tree diagram,
% \textbf{Add examples to each of the learning}
\resizebox{\linewidth}{!}{%
\begin{center}
    \begin{forest}
  [Machine Learning
    [Supervised \\ Learning
     [Classification 
       [Binary \\ classification]
       [Multi \\classification]
     ]
     [Regression
       [Regression \\ modeling
         [\textit{Ensembling}]
       ]
     ]
    ]
    [Unsupervised \\ Learning
      [Clustering
      [K-means Clustering]
      ]
      [Anomaly detection
        % [V$'$
        %   [V
        %     [\textit{is}]
        %   ]
        %   [AP
        %     [Deg
        %       [\textit{extremely}]
        %     ]
        %     [A$'$
        %       [A
        %         [\textit{straightforward}]
        %       ]
        %       [CP
        %         [\textit{to wield}, roof]
        %       ]
        %     ]
        %   ]
        % ]
      ]
    ]
    [semi-supervised\\ learning
    ]
    [reinforcement \\learning
    ]
  ]
\end{forest}
\end{center}
}%

\section{Evaluating models}
In machine learning, the ultimate goal is to achieve models that generalize, i.e., that perform well on never-before-seen data, and over fitting is the central obstacle. To do this, splitting the available data is very crucial. Training, validation, and testing are the partitions  
needed, so during the training phase, the model trains with the training data and test with the validation data, and when the model is ready, it is tested one last time with the test data, briefly explained by \autoref{fig:my_label_09876} \\
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{Figure/ml__12.png}
    \caption{Splitting data to be used in the training phase. Image source: [Chollet, 2017]}
    \label{fig:my_label_09876}
\end{figure}
The reason why three (and not two) datasets are used is that tuning in the model configuration is required. The parameters that can be tuned in a machine learning model are called hyperparameters\cite{https://doi.org/10.48550/arxiv.1904.11829,https://doi.org/10.48550/arxiv.1412.3555}. Hyperparameter values can be changed to control the learning process. Meanwhile, the value of other parameters, like node weights, are derived by training and cannot be adjusted. Hyperparameter tuning is done with the feedback of the model performance in the validation data and further applied under an unseen test dataset. This is a basic approach for each machine learning model, and we will apply the same over the neural network model in the next section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%DNNNNNNNNNNNNNDNNNDNNNDNNNNDNNDNNNDNNDNDNNDNDNNDNDNNNDNNNNDNNNDNDNDNDNDNDNDNDNDNDNNDNDNDNDNNDND
\section{Deep Neural Network}
Machine learning works very efficiently on numerous problems but sometimes fails to excel in a few specific cases, which appears very easy for humans. For example, classifying an image as a cat or dog or distinguishing audio clips as a male or female voice,etc\dots The machine fails to identify this type of problem of image classification or video segregation and other unstructured data types, which are easy for us. There come to the idea of a deep neural network(DNN), where the idea is to mimic the human brain's biological process, which is composed of billions of neurons connected to each other and to adapt and learn new things\cite{LeCun2015}. \\ 


A simplified version of Deep Neural Network can be represented as a hierarchical (layered) structure of neurons (compared with the neurons in the human brain) with connections to other neurons. These neurons pass a message or signal to other neurons based on the received input and form a complex network system that learns with some feedback mechanism. The following diagram(\autoref{fig:my_label_hu}) represents an 'N' layered Deep Neural Network.\\

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{Figure/1_ML_report.png}
    \caption{A Deep Neural Network with N hidden layers}
    \label{fig:my_label_hu}
\end{figure}
The input data is provided to the neurons into the first layer (not hidden), which subsequently provides an output to the neurons within the next layer and so on and finally provides the final output. These outputs might be a prediction such as Yes or No (just as we represent in probability). Each layer can consist of one or many neurons, and each of them will be computed with the help of a small function, i.e., activation function. The activation function takes the signal from the previous layers and passes it further to the next connected neurons. There is a threshold value corresponding to each activation function, where the output is passed when it is above the threshold value; else, it gets ignored. The connection between two neurons of successive layers always passes with an associated weight. \\
 
 The weights have a very important role to play in the correct prediction from the model. This weight defines the influence of the input on the output for each layer. We provide initial weight to the model randomly, but during the training, these weights get updated iteratively to learn to predict a suitable output.\\
 In a neural network, the initial weights would be provided by us as a random number, but during the model training, these weights are updated iteratively by themselves to learn to predict a correct output. The network also depends on the learning mechanism (optimizer)\cite{https://doi.org/10.48550/arxiv.1912.08957}, which helps the neural network to update its weights (that were randomly initialized) to a more suitable weight that aids in the correct prediction of the outcome.  To update its weight for the connections, the mathematical algorithm used is called backpropagation\cite{118638}. The iteration of the process several times, with more and more data, helps the model to update its weights appropriately. By iterating the process several times, with the help of more data, the networks update the weights appropriately to create a system where the system can take a decision for predicting output based on rules which the model created for itself with the help of weights and connections.\\


Deep learning is efficient to work with a large amount of data which has made it popular in the last few years; a few of the popular choices for the frameworks of deep learning in python are:-\\
 \begin{itemize}
     \item Low-level frameworks 
         \begin{itemize}
             \item TensorFlow
             \item MxNet
             \item PyTorch
         \end{itemize}
      \item High-level frameworks
         \begin{itemize}
             \item Keras (uses TensorFlow as a backend)
              \item Gluon (uses mxnet as a backend)
         \end{itemize}
 \end{itemize}

Schematically, a neural network(unit, node) layer can be represented as in below \autoref{fig:my_label_3}. 
\begin{figure}[H]
    \centering
    \includegraphics[scale =0.5]{Figure/ml__1.png}
    \caption{Basic structure of a neural network\cite{https://doi.org/10.48550/arxiv.1506.00619}}
    \label{fig:my_label_3}
\end{figure}




The function representing the neural network can be expressed as:

\begin{equation}\label{eq:eq1}
a= f(z) = f(\sum_{j=1}^{j=m} x_jw_j +w_0) = f(w^T x+w_0)
\end{equation}
A non-linear function of an input vector x $\in \mathbb{R}^m$ note to a single output value  a $\in$R . It is parameterized by a vector of weights  $(w_1, w_2,....w_m)\in \mathbb{R}^m$  and an offset or threshold  $w_0\in \mathbb{R}^m$ . In order for the neuron to be non-linear, we also specify an activation function  $f:\mathbb{R} \to \mathbb{R}$ , which can be the f(x)=x(linear function), or can also be any other non linear function, such as ReLU, tanh, etc., which is differentiable. 
\\
Before thinking about a whole network,let us consider how to train a single unit. \\
Given a loss function L(guess, actual) and a dataset $\{(x^{(1)},y^{(1)}),.....,(x^{(n)},y^{(n)})\}$, we can do (stochastic) gradient descent, adjusting the weights w, $w_0$ to minimize the equation \cite{LeCun2015}

\begin{equation} \label{eq:eq2}
    J(W, W_0) = \sum_{i=1} ^n L(f(x^{(i)}:W),y^{(i)})
\end{equation}
where f is the output of our neural net for a given input.

% \textbf{Provide example here :linear logistic classifiers (LLC)
% with NLL loss and regressors with quadratic loss! The activation function for the LLC is
% f(x) = $\sigma$(x) and for linear regression it is simply f(x) = x. \\
% Just for a single neuron, imagine for some reason, that we decide to use activation function f(z) = $e^z$ and loss function $L(g, a) = (g-a)^2$. Derive a gradient descent update for w and $w_0$.}

%%%%%%%%%%%%%%%%%%DO from here%%%%%%%%%%%%%%%%%%%
\subsection{Networks}


Now, we’ll try to train the network with stacking multiple neurons together to form a network. A neural network in general takes input $x \in \mathbb{R}^m$ and generates an output $\alpha \in \mathbb{R}^n $.  It is constructed with the help of multiple neurons; the inputs of each neuron might be elements of x and/or outputs of other neurons. The outputs are generated by n output units.
Here, for the training of data, we will only consider feed-forward networks. In a feed-forward network, we can think of the network as defining a function-call graph that is acyclic. For the simplicity in software and analysis, we usually organize networks into layers. A \textbf{layer} can be defined as a group of neurons which are connected to each other parallelly (as in \autoref{fig:my_label_hu}): The input of a hidden layer depends on the output of previous layer(hidden or input layer); and the output from the layers are input to the neurons in the subsequent next layer. We will start to describe about the model with a single layer and further go on to the case of multiple layers.

\subsubsection{Single-layer}
A layer is a set of unrelated entities, as  just described.  The  inputs to each unit in the layer are said to be fully connected, as shown in the figure below.  It's the same. The layer has an input x $\in\mathbb{R}^m$ and an output (also known as activation) $\alpha\in\mathbb{R}^n$
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{Figure/ml_2.png}
    % \caption{Caption}
    \label{fig:my_label}
\end{figure}
Since each unit layer has a vector of weights and a single offset, we can consider the weights of
the whole layer as a matrix, W, and the collection of all the offsets as a vector $W_0$. If we
have m inputs, n units, and n outputs, then,
\begin{itemize}
    \item  W is an m $\times$ n matrix
     \item $W_0$ is an n$\times$  1 column vector,
     \item X, the input, is an m $\times$  1 column vector,
     \item Z = W$^T$X + $W_0$, the pre-activation, is an n$\times$  1 column vector,
\end{itemize}
and the output vector will be\\
\begin{equation*}
    A = f(Z) = f(W^Tx +W_0)
\end{equation*}

\subsubsection{Many layers}

A single neural network generally consists of multiple layers, where the output of the previous layer feeds as input to the next layer..\\
 We will use l to name a layer and 
let $m^l$ be the  number of inputs to the layer and $n^l$ be the number of outputs from the layer. Then, $W^l$ and $W^l _0$ are of shape $m^l \times n^l$, respectively. Let $f^l$ be the activation
function of layer  $ \ell$ . Then, the pre-activation outputs are the $n^l \times$ 1 vector, such that,
\begin{equation*}
    Z^l = {W^l}^T A^{l-1} + W_0^l
\end{equation*}
and the activation function outputs are simply the $n^l \times 1$ vector
\begin{equation*}
    A^l = f^l(Z^l)
\end{equation*}
We will use this structural diagram to organize our algorithmic thinking and implementation different parameters in deep neural network.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{Figure/ml__3.png}
    % \caption{Caption}
    \label{fig:my_label}
\end{figure}

As we saw here how a model of neural network function, the training and output from any model depends on the type of non linear functions Z, i.e. activation function we use in between the layers. Now, the question arises, how many types of activation functions are there?, and how we choose which one will be suitable for our model? We will addresses all these issues over the next section.

\subsection{ Activation function}
\label{subsection:Activationfunction}
There are three types of neural networks activation functions\cite{https://doi.org/10.48550/arxiv.1811.03378}:
\subsubsection{Binary Step Function}
Binary step function depends on a threshold value which decides whether a neuron should be activated or not. This can be represented using the equation \autoref{eq:2}

\begin{equation} \label{eq:2}
   f(x) =  \begin{cases} 
      0 for x < 0 \\
      1 for x \geq 0 
   \end{cases}
\end{equation}
The output of the equation can be represented graphically as,
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{Figure/binary.png}
    % \caption{Caption}
    \label{fig:my_label}
\end{figure}
The binary activation function is not always useful; a few of its limitations are:
\begin{itemize}
    \item We cannot use this activation function to provide us an output of multiclass problems, as it has only two labels of output.
    \item The gradient of the step function is zero, which causes a restriction in the back propagation process.
\end{itemize}

 
\subsubsection{Linear Activation Function}
Also known as identity Function, i.e. it can be represented using equation, and fig. below,
\begin{equation}
    f(x) = x
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{Figure/linear.png}
    % \caption{Caption}
    \label{fig:my_label_02wwe}
\end{figure}

However, a linear activation function also has these two major problems :

\begin{itemize}
    \item It’s impossible to use back propagation as the gradient of the function is always a constant and has no relation with the input x.
    \item After use of linear activation function, without having dependence on a number of layers, the last layer will still be a linear function of the first layer. A linear activation function turns the neural network model into just one layer, which leads to model collapse.
\end{itemize}

 
\subsubsection{Non-Linear Activation Functions}

Non-linear activation functions solve the above limitations possessed by both linear activation functions and binary activation function as the derivative are possible and also related to the inputs, thus backpropogation are allowed here.

Few non-linear activation functions are:-\\
\textbf{Sigmoid / Logistic Activation Function}\\
Input is any real value(i.e. x $\in$ $\mathbb{R}$)and outputs $\in$ [0,1].\\
Mathematically, it can be represented as:\\
\begin{equation}
    f(x) = \frac{1}{1+e^{-x}}
\end{equation}
The output of the above equation is,
\begin{figure}[H]
    \centering
    \includegraphics{Figure/sigmoid.png}
    % \caption{Caption}
    \label{fig:my_label}
\end{figure}

\textbf{Tanh Function}\\
Tanh function is very similar to the sigmoid/logistic activation function, with only difference in the output range of -1 to 1. 
Mathematically, it can be represented as;
\begin{equation}
    f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\end{equation}
And, graphically, it can be represented as,\\
\begin{figure}[H]
    \centering
    \includegraphics{Figure/Tanh.png}
    % \caption{Caption}
    \label{fig:my_label}
\end{figure}


Tanh activation function gives advantages as the output of the tanh activation function is Zero centered; thus, we can easily map the output values as strongly negative, neutral, or strongly positive. Another advantage is that the value of the hidden layers in a neural network has its values  between -1 to 1, and the mean for the hidden layer comes out to be 0 or very close to it. Therefore, it helps in centering the data and makes learning for the next layer much easier\cite{227257}.

\textbf{ReLU} \\
ReLU stands for Rectified Linear Unit. ReLU has a derivative function, despite seems like linear function. It allows backpropagation and also simultaneously making it computationally efficient. The ReLU function does not activate all the neurons at the same time. The neurons get deactivated when its output is less than 0, that is,
\begin{equation}
    f(x) = max(0,x) = \begin{cases} 
      0 & z < 0 \\
      z & otherwise 
   \end{cases}
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics{Figure/RELU.png}
    % \caption{Caption}
    \label{fig:my_label}
\end{figure}


It is the most commonly used activation  function due to following unique features. Since ReLU activation function activate only a certain number of neurons, and it help them to do computation efficiently  in comparison to the sigmoid and tanh functions.\cite{https://doi.org/10.48550/arxiv.1811.03378} The ReLU activation function also increase or decrease the rate of convergence towards global minimum of the loss function due to its linear, non saturating property\cite{https://doi.org/10.48550/arxiv.1803.08375}.

\textbf{Softmax}\\
It is used to calculate the relative probabilities. Like sigmoid activation function, the SoftMax function also returns the probability of each class.
It is the most commonly used activation function for the last or output layer of the neural network in the case of multi-class classification\cite{https://doi.org/10.48550/arxiv.1811.03378}. 
Mathematically it can be represented as:
\begin{equation}
    softmax(z_i) = \frac{exp(z_i)}{\sum_j exp(z_j)}
\end{equation}
\begin{figure}[H]
    \centering
    \includegraphics{Figure/softmax.png}
    % \caption{Caption}
    \label{fig:my_label}
\end{figure}
Here, we saw a few common activation functions, but how we can choose activation function for the given problem? 

\subsubsection{How to choose activation function for Hidden Layers}
The choice of activation function in between the layers depends on the type of problem we have, as it is summarized in the tree below.
 \begin{center}
    \begin{forest}
      [
      Network type?
       [Multilayer Perceptron
        [ReLU \\ Activation]
       ]
       [Convolutional Neural Net
        [ReLU \\ Activation]
         ]
        
       [Recurrent Neural Net
       [sigmoid Activation]
       [Tanh Activation]
       ]
      ] 
    %   [Multi-class\\ cross-Entropy \\Loss \\ Functions
    %   [Multi-class\\ Cross-Entropy\\ Loss]
    %   [Sparse Multiclass \\cross-Entropy \\loss]
    %   [Kullback\\ Leibler\\ Divergence \\Loss]
    %   ]
      
    \end{forest}
 \end{center}



\subsubsection{How to choose activation function for output layers}
% \begin{figure}[H]
%     \centering
%     \includegraphics[scale=0.5]{Figure/ml__4.png}
%     % \caption{Caption}
%     \label{fig:my_label}
% \end{figure}
The activation function used in the last layer of the neural network depends on the type of output required and  the type of prediction problem you are trying to solve. This can be summarized from the tree below.\cite{ML_1}.
\begin{itemize}
    \item \textbf{Regression}: one node, Linear Activation
    \item \textbf{Binary Classification}:one node per class, Softmax Activation
    \item \textbf{Multiclass Classification}:One node per class, softmax activation
    \item \textbf{Multilabel Classification}: One node per class, sigmoid activation
\end{itemize}

 \begin{center}
    \begin{forest}
      [
      Problem type?
       [Classification
        [Binary \\ Classification
         [Sigmoid Activation]
        ] 
        [Multiclass \\ Classification
         [Softmax Activation]
         ]
        [Multilabel \\ Classification
        [Sigmoid Activation]
        ]
       ]
       [Regression
       [Linear Activation]
       ]
    %   [Multi-class\\ cross-Entropy \\Loss \\ Functions
    %   [Multi-class\\ Cross-Entropy\\ Loss]
    %   [Sparse Multiclass \\cross-Entropy \\loss]
    %   [Kullback\\ Leibler\\ Divergence \\Loss]
    %   ]
      ]
    \end{forest}
 \end{center}

% \begin{figure}
%     \centering
%     \includegraphics[scale=0.5]{Figure/ml__5.png}
%     % \caption{Caption}
%     \label{fig:my_label}
% \end{figure}















% Source
% \url{https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/}
% \url{https://www.v7labs.com/blog/neural-networks-activation-functions}
% ReLUs are especially common in internal (“hidden”) layers, and sigmoid activations are
% common for the output for binary classification and softmax for multi-class classification.(\url{https://openlearninglibrary.mit.edu/assets/courseware/v1/9c36c444e5df10eef7ce4d052e4a2ed1/asset-v1:MITx+6.036+1T2019+type@asset+block/notes_chapter_Neural_Networks.pdf}
\section{Training of Neural Network}



\subsection{Error backpropagation}
Train the neural network using the steepest descent method. You can use the batch gradient descent method, which sums the gradients of all points, or the  stochastic gradient descent method (SGD), which takes small steps with respect to the gradient after seeing one point at a time\cite{10.5555/2968826.2968922}.
% \textbf{Include theorem from} \url{https://openlearninglibrary.mit.edu/assets/courseware/v1/d81d9ec0bd142738b069ce601382fdb7/asset-v1:MITx+6.036+1T2019+type@asset+block/notes_chapter_Gradient_Descent.pdf}\\

we will always compute the gradient of the loss function with respect
to the weights for a particular value of (x, y)\cite{10.5555/2968826.2968922, https://doi.org/10.48550/arxiv.1811.03378, https://doi.org/10.48550/arxiv.1506.00619}. That tells us how much change is needed in the
weights, in order to reduce the loss experienced on this particular training. Let us understand with this example.\\
First, let’s us calculate and observe how the loss depends on the weights in the final layer, $W^L$ remembering
that our output is $A^L$, and using the shorthand loss to stand for Loss((f(x; W), y) which
is equal to Loss($A^L$, y), and finally that $A^L$ = $f^L$($Z^L$) and $Z^L$ = ${W^L}^T$ $A^{L-1}$, we can apply the chain  rule as:
\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.3]{Figure/ml__6.png}
    % \caption{Caption}
    \label{fig:my_label}
\end{figure}
Here, we need to be little careful with the dimensions, and here we can note that it is true for any $ \ell$, including $ \ell$ = L\\
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{Figure/ml__7.png}
    % \caption{Caption}
    % \label{fig:my_label}
\end{figure}
If  the chain rule can be applied repeatedly, we get the following equation for the loss gradient  with respect to the preactivation function of the first layer:
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{Figure/ml__8.png}
    % \caption{Caption}
    \label{fig:my_label}
\end{figure}


Here,
\begin{itemize}
    \item $\frac{\partial loss}{\partial A^L}$ is $n^L \times 1$ and depends on the particular loss function you are using.
    \item $\frac{\partial Z^l}{\partial A^{l-1}}$ is $m^L \times n^L$ and is just $W^l$
    \item $\frac{\partial A^l}{\partial Z^l}$ is $n^L \times n^L$. Each element $\alpha_i ^l = f^l(z_i ^l)$. This means that $\frac{\partial \alpha_i ^l}{\partial z_j ^l}$ = 0 whenever i$\neq$ j. So, the off-diagonal elements of $\frac{\partial A^l}{\partial Z^l}$ are all 0, and the diagonal elements are $\frac{\partial \alpha_i ^l}{\partial z_j ^l}$= ${f^l}^'(z_j ^l)$
\end{itemize}
We can write the above equation as,
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{Figure/ml__09.png}
    % \caption{Caption}
    \label{fig:my_label}
\end{figure}

This general process is called error back-propagation. The general idea is that we will do a forward
pass to compute all the $\alpha$ and z values at all the layers. Then, we can start work backward direction and compute the gradient of the loss with respect
to the weights in every layer, starting at last layer L and going back to layer 1, in this way model can update its weight, as can be shown below,\\
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{Figure/ml__10.png}
    % \caption{Caption}
    \label{fig:my_label}
\end{figure}




Now, How we can do stochastic gradient descent training on a feed-forward neural network. This is the pseudo code to apply stochastic gradient descent,

% \begin{algorithm}
% \caption{\textit{SGD-NEURAL-NET(D_n,T, L, ($m^1$, ...$m^L$), ($f^1$,......,$f^L$))}}\label{alg:cap}
% \begin{algorithmic}
% \for {l=1 to L}
% \State $W_{ij} ^l ~ Gaussian(0, \frac{1}{m^l})$
% % \ $y = x^n$
% % \State $y \gets 1$
% % \State $X \gets x$
% % \State $N \gets n$
% % \While{$N \neq 0$}
% % \If{$N$ is even}
% %     \State $X \gets X \times X$
% %     \State $N \gets \frac{N}{2}$  \Comment{This is a comment}
% % \ElsIf{$N$ is odd}
% %     \State $y \gets y \times X$
% %     \State $N \gets N - 1$
% % \EndIf
% % \EndWhile
% \end{algorithmic}
% \end{algorithm}

\begin{lstlisting}[language=Python]
def SGD(f, theta0, alpha, num_iters):
    """
      Arguments:
      f -- the function to optimize, it takes a single argument
            and yield two outputs, a cost and the gradient
            with respect to the arguments
      theta0 -- the initial point to start SGD from
      num_iters -- total iterations to run SGD for
      Return:
      theta -- the parameter value after SGD finishes
    """
    start_iter = 0
    theta = theta0
    for iter in xrange(start_iter + 1, num_iters + 1):
        _, grad = f(theta)
  
        # there is NO dot product ! return theta
        theta = theta - (alpha * grad)
\end{lstlisting}



\section{Loss functions }
Now, the choice of a loss function for a particular problem is a very tedious task. We can take a rough idea about the loss function from this tree,\\
\resizebox{\linewidth}{!}{%
 \begin{center}
    \begin{forest}
      [
      Loss Function
       [Regression \\Loss\\ functions
        [Mean \\ Squared \\Error loss]
        [Mean\\ Squared \\Logarithmic \\Error loss]
        [Mean\\ absolute\\ Error\\ Loss]
       ]
       [Binary \\Classification\\ Loss \\Functions
       [Binary \\Cross- Entropy]
       [Hinge\\ Loss]
       [Squared\\ Hinge Loss]
       ]
       [Multi-class\\ cross-Entropy \\Loss \\ Functions
       [Multi-class\\ Cross-Entropy\\ Loss]
       [Sparse Multiclass \\cross-Entropy \\loss]
       [Kullback\\ Leibler\\ Divergence \\Loss]
       ]
      ]
    \end{forest}
 \end{center}
}

Like activation function, loss function also take different assumptions about the range of inputs it will take depending on the type of problem in hand. While designing the model of neural network, it become important to make things to fit well together. As, in particular, we will think about matching loss function with activation function of the last layer, $f^L$\cite{https://doi.org/10.48550/arxiv.1702.05659}. This hypothesis can be infer from the table below, \autoref{tab:my_label_12}.\\
\begin{table}[h]
    \centering
    \begin{tabular}{ccc} \hline
   \textit{   Loss }  & \textit{ $f^L$} & \textit{Comments}\\ \hline
      squared   &  Linear &  --\\
      hinge  &  Linear &  for "maximum-margin" classification\\
      NLL &   Sigmoid &   negative log likelihood loss. Useful to \\ 
      &    &       train classification problem with C classes.\\
      NLLM & softmax & -- \\
      
    \end{tabular}
    \caption{Few loss functions corresponding to the last layer/ output layer activation function. There are many different types of the loss function following the kind of problem, whether it belongs to classification or regressions. }
    \label{tab:my_label_12}
\end{table}
There are also other loss function we have used to make our model better, such as "BinaryCrossentropy", "CategoricalCrossentropy", and
"SparseCategoricalCrossentropy" belonging to probabilistic losses. Also, "mean\_squared\_error", "mean\_absolute\_error", and "MeanSquaredError" belonging to regression losses.
 
\subsubsection{Two-class classification and log likelihood}

% For classification, the natural loss function is 0-1 loss, but we have already discussed the
% fact that it’s very inconvenient for gradient-based learning because its derivative is discontinuous. \\
For binary classification problems, which are useful in separation of signal and background, Hinge loss gives us another way, to make a smoother objective, penalizing the margins of the labeled points relative to the separator. The hinge loss is defined to be
\begin{equation*}
    L_h(guess, actual) = max(1-guess.actual,0)
\end{equation*}
when actual $\in$ \{+1, -1\}
Using hinge loss with  squarednorm regularization, the learning process attempts to  find  the delimiter with the largest margin for the dataset. This optimization setting is called \textbf{Support Vector Machine}. Popular for its square shape that makes it easy to optimize SVMs\cite{https://doi.org/10.48550/arxiv.1511.08861}.

\subsubsection{Multi-class classification and log likelihood}
Multi-class classification with total of K classes, where the training label is represented with the one-hot vector $y = [y_1,...., y_k]^T$, where $y_k$= 1 if the example is of class k.Assume that our network uses softmax as the activation function (Which is most commomly used activation function for multi classification) in the last layer, so that the output is
$\alpha = [\alpha_1,....,\alpha_k]^T$, which represents a probability
distribution over the K possible classes. Then, the probability that our network predicts
the correct class for this example is $\Pi_{k=1} ^N \alpha_k ^y$ and the log of the probability that it is correct is $\sum_{k=1} ^k y_k \log \alpha_k$,so
\begin{equation*}
    L(guess, actual) = - \sum_{k=1} ^K actual_k.log(guess_k)
\end{equation*}





\section{Optimizing neural network parameters}
As neural networks consists of many parameters, our ultimate goal to minimize the loss function. The optimization can be done with help of standard gradient-descent softwares, but here, we can take advantages of the structure of the loss functions to improve optimization. The structure of loss function as a sum over terms, one training data point, help us to consider stochastic gradient methods. In this section, we will try to consider some alternative strategies for organizing training, and also to make easier to handle the step-size parameter\cite{8285338}.\\


\subsection{Batches}

Lets us assume we have an objective function of the form\\
\begin{equation*}
    J(W) = \sum_{i=1} ^n L(h(x^{(i)};W),y^{(i)})
\end{equation*}
Where h is the function calculated by the neural network and W represents all the weight matrices and vectors in the network. \\
 Use update rules when performing batch gradient descent.\\
\begin{equation*}
    W := W-\eta \nabla_W J(W), 
\end{equation*}
which is equivalent to 
\begin{equation*}
     W := W-\eta \nabla_W \sum_{i=1} ^n L(h(x^{(i)};W),y^{(i)})
\end{equation*}
Thefore, we add up the gradient of loss after each training point, with respect to W, and then take a step in the negative direction of the gradient to minimize the loss.\\


A more effective optimization strategy  is to use a mini-batch to take an "average" between the batch and the stochastic gradient descent method. For mini-batch of size k,  k different data points are evenly and randomly selected from the dataset and weight updates are performed based solely on the contribution to the gradient.
\begin{equation*}
    W \leftarrow W-\eta \nabla_W \sum_{i=1} ^n L(h(x^{(i)};W),y^{(i)})
\end{equation*}
Most neural network software packages are set up to do mini-batches\cite{25}.\\

To select k unique data points at random from a large dataset is computationally difficult. An alternative strategy, if we have an efficient procedure for randomly shuffling the data set (or randomly shuffling a list of indices into the data set) is to operate
in a loop, roughly as follows:\\
\begin{algorithm}
\caption{\textit{MINI-BATCH-SGD(NN, data, k)}}\label{alg:cap}
\begin{algorithmic}
\State $n = length(data)$
\While{not done:}
     \State $RANDOM-SHUFFLE(data)$
     \For {i=1 to $\frac{n}{k}$}
           \State $BATCH-GRADIENT-UPDATE(NN,data[(i-1)k:ik])$
% \Require $n \geq 0$
% \Ensure $y = x^n$
% \State $y \gets 1$
% \State $X \gets x$
% \State $N \gets n$
% \While{$N \neq 0$}
% \If{$N$ is even}
%     \State $X \gets X \times X$
%     \State $N \gets \frac{N}{2}$  \Comment{This is a comment}
% \ElsIf{$N$ is odd}
%     \State $y \gets y \times X$
%     \State $N \gets N - 1$
% \EndIf
% \EndWhile
\end{algorithmic}
\end{algorithm}
then, we can easily divide the dataset into k mini batches.

\subsection{ Adaptive stepsize}

Picking the value for $\eta$ is difficult and time-consuming. As our networks become deep (with increase in the numbers of layers) we can find that magnitude of the gradient of the loss with respect the weights in the last layer,$\frac{\partial loss}{\partial W_L}$,may have significant differences from the gradient of the loss with respect to the weights in the first layer $\frac{\partial loss}{\partial W_1}$.\\

The output gradient is further multiplied by all the weight matrices of the network and is “fed
back” through all the derivatives of the activation functions. This can lead to a general problem
of \textbf{exploding or vanishing gradients}, in which the back-propagated gradient is either too big
or small.

\subsubsection{Running averages}


It is a computation strategy for estimating a weighted average for a sequence of data. Let us take data sequence be $\alpha_1, \alpha_2,......;$ then we define a sequence of running average values, $A_0, A_1, A_2,....$ using the equations
\begin{equation*}
      A_0 = 0
\end{equation*}
\begin{equation*}
         A_t = \gamma_t A_{t-1} + (1-\gamma_t)\alpha_t
\end{equation*}
where $\gamma_t$ $\in$ (0,1). If $\gamma_t$ is a constant, then this is a moving average, in which
\begin{equation*}
         A_T = \gamma A_{T-1} + (1-\gamma)\alpha_T
\end{equation*}
\begin{equation*}
       = \gamma(\gamma A_{T-2} +(1-\gamma)\alpha_{T-1}) +(1-\gamma)\alpha_T
\end{equation*}
\begin{equation*}
     = \sum_{t=0} ^T \gamma^{T-t}(1-\gamma)\alpha_t
\end{equation*}
So, you can see that inputs $\alpha_t$ closer to the end of the sequence have more effect on $A_t$ than early inputs.\\


\subsubsection{Momentum}
You can use this method as well as a moving average to explain a strategy for calculating $\eta$. The easiest way is Momentum. This method "averages" the latest gradient updates and retrieves them when at least  that component of the motion vibrates in one direction. For momentum, we have\\
\begin{equation*}
      V_0 = 0
\end{equation*}
\begin{equation*}
         V_t = \gamma V_{t-1} + \eta \nabla_W J (W_{t-1})
\end{equation*}
\begin{equation*}
         W_t =  W_{t-1} - V_t
\end{equation*}
This does not look like the adaptive step size method. But if we let it do 
 If $\eta = \eta`(1 \gamma)$, the rule is similar to using the increment $\eta'$ to update to a moving average of the gradient using the parameter $\gamma$. I can see.:
\begin{equation*}
    M_0 = 0
\end{equation*}
\begin{equation*}
           M_t = \gamma M_{t-1} + (1-\gamma) \nabla_W J (W_{t-1})
\end{equation*}
\begin{equation*}
         W_t =  W_{t-1} - \eta'M_t
\end{equation*}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{Figure/ml__11.png}
    \caption{The red arrows show the change in the value of weight after one step of mini-batch gradient descent with use of momentum. The blue points show about the direction of the gradient with respect to
the mini-batch at each step. The Momentum smooths the path taken towards the local
minimum and further leads to faster convergence.\cite{21},\cite{25}}
    \label{fig:my_label}
\end{figure}

\subsubsection{Adadelta}
The idea here is to perform a large step in  the space where J(W) is almost flat (because there is no risk of the step becoming too large due to the large gradient), and if it is steep, perform a small step. That is. Apply this idea to any weight and finally get a method called Adadelta. 
 Again, the weights are indexed by the layer that contains the input and output units. Let $W_j$ be any weight in the network.\\
\begin{equation*}
    g_{t,j} = \nabla_W J (W_{t-1})_j
\end{equation*}
\begin{equation*}
       G_{t,j }= \gamma G_{t-1, j} + (1-\gamma) g_{t,j}^2
\end{equation*}
\begin{equation*}
    W_{t,j} = W_{t-1,j}- \frac{\eta}{\sqrt{G_{t,j}+\epsilon}}g_{t,j}
\end{equation*}
% The sequence $G_{t,j}$ is a moving average of the square of the jth component of the gradient.
% We square it in order to be insensitive to the sign—we want to know whether the magnitude is big or small. Then, we perform a gradient update to weight j, but divide the step
% size by $\sqrt{G_{t,j}+\epsilon}$,which is larger when the surface is steeper in direction j at point $W_{t-1}$ in weight space; this means that the step size will be smaller when it’s steep and larger when
% it’s flat.

\subsubsection{Adam}

Adam has become the most common and default method of managing step sizes neural networks. The moving averages of the gradient
and squared gradient, which estimates the mean and variance of the gradient for weight j:
\begin{equation*}
     g_{t,j} = \nabla_W J (W_{t-1})_j
\end{equation*}
\begin{equation*}
        m_{t,j} = B_1 m_{t-1,j} + (1-B_1) g_{t,j}
\end{equation*}
\begin{equation*}
        v_{t,j} = B_2 v_{t-1,j} + (1-B_2) g_{t,j}^2
\end{equation*}

If we initialize $m_0$ = $v_0$ = 0, then there will always be bias(slightly too small). So we will correct the bias by defining,
\begin{equation*}
    \hat{m}_{t,j} = \frac{m_{t,j}}{1-B_1^t}
\end{equation*}
\begin{equation*}
    \hat{v}_{t,j} = \frac{v_{t,j}}{1-B_2^t}
\end{equation*}
\begin{equation*}
    W_{t,j} = W_{t-1, j} - \frac{\eta}{\sqrt{ \hat{v}_{t,j}+\epsilon}}\hat{m}_{t,j} 
\end{equation*}
Adam did not have a huge effect on the result after making small changes in the model, which makes it an efficient method.
\section{Regularization}
So far, we've only talked about how to optimize  training data loss. As mentioned earlier, the risk of overfitting remains. This overfitting problem can be fixed by increasing the  data size. This is the case today when deep neural networks use large amounts of data. Nonetheless, there are several strategies for regularizing  neural networks, and sometimes they can be important. This can be done by implementing an early stop. This is to train in a training set and  evaluate  the current W loss in the validation set (through the entire training set or in some cases more often) at each epoch. Here, it is observed that the loss of the training set decreases fairly consistently at any number of iterations, the loss of the validation set decreases first, and then begins to increase. 
 once again. If you find that the validation loss increases systematically, you can stop training the model and return the weight with the fewest validation errors. Another easy way is to penalize  all weight criteria. This method is known as weight loss,
\begin{equation*}
    J(W) = \sum_{i=1}^n Loss(NN(x^{(i)}), y^{(i)};W) + \lambda ||W||^2
\end{equation*}
we end up with an update of the form
\begin{equation*}
    W_t = W_{t-1}(1-\lambda_\eta) - \eta (\nabla_W Loss(NN(x^{(i)}), y^{(i)};W_{t-1}))
\end{equation*}

This rule has the form of first “decaying” $W_{t-1}$ by a factor of (1 -$\lambda \eta$) and then taking a gradient step.
Other few methods are:-
% \subsection{Methods related to ridge regression}
% Early stopping is the easiest to implement and is in fairly common use. The idea is
% to train on your training set, but at every epoch (pass through the whole training set, or
% possibly more frequently), evaluate the loss of the current W on a validation set. It will
% generally be the case that the loss on the training set goes down fairly consistently with
% each iteration, the loss on the validation set will initially decrease, but then begin to increase
% again. Once you see that the validation loss is systematically increasing, you can stop
% training and return the weights that had the lowest validation error.\\
% Another common strategy is to simply penalize the norm of all the weights,This method is known as weight decay, because when we take the gradient of the objective
% \begin{equation*}
%     J(W) = \sum_{i=1}^n Loss(NN(x^{(i)}), y^{(i)};W) + \lambda ||W||^2
% \end{equation*}
% we end up with an update of the form\\
% \begin{equation*}
%     W_t = W_{t-1}(1-\lambda_\eta) - \eta (\nabla_W Loss(NN(x^{(i)}), y^{(i)};W_{t-1}))
% \end{equation*}

% This rule has the form of first “decaying” $W_{t-1}$ by a factor of (1 -$\lambda \eta$) and then taking a gradient step.


\subsection{Dropout}
The contains a simple idea, it suggest rather than perturbing the data every time we train, we will make changes in the
network. Here, we will randomly, from each dataset, after selecting a unit layer prohibit them from participating  in the training. Therefore, all  units It's a  kind of overall responsibility for  the correct answer  and can't be trusted performs all required calculations for each small subset of weights. This also tends make your network more resilient to data failures\cite{JMLR:v15:srivastava14a}.\\
When you have finished training and want to use the network to make predictions,  multiply all weights by p to get the same average activation level. Allow training forward pass\\
\begin{equation}
    \alpha^l = f(z^l) * d^l
\end{equation}
where * denotes component-wise product and $d^l$ is a vector of 0’s and 1’s drawn randomly
with probability p. The backwards pass depends on $\alpha^l$ so we do not need to make any
further changes to the algorithm.
Another modern alternative method to dropout, is batch normalization.
\subsection{Batch Normalization}

Here, idea is when training is done with mini-batches, the idea is to standardize the input values for each mini-batch, subtracting off the mean and dividing by the standard deviation of each input dimension. This gives us similar effect to adding noise and dropout. Each mini-batch of data ends up getting mildly perturbed, which prevents the network from exploiting very particular values of the data points\cite{https://doi.org/10.48550/arxiv.1502.03167}.
% So, when training with mini-batches, the idea is to standardize the input values for each mini-batch, subtracting off the mean and dividing by the standard deviation of each input dimension. This Batch normalization ends up having a regularizing effect for similar reasons that adding noise and dropout do: each mini-batch of data ends up being mildly perturbed, which prevents the network from exploiting very particular values of the data points.\\

\section{Reciever Operating Characteristic(ROC) Curve}
It is used for the performance measurement for the classififcation problems at various thresholds. ROC is a probability curve and Area-Under Curve(AUC) represents the degree or measure of separability\cite{BRADLEY19971145}. To measure the curve output after training, defined with respect to a given class \textit{C}.It tells how lots version is able to distinguishing among classes. Given a point x and model that outputs a P(C|x) probability that x belongs to the class C. Given T , a threshold, x belongs to C if and only if P(C|x) $\geq$ T . If T = 1, a point is labeled as belonging to class, C only if the model is 100\% sure. If T = 0, every point is labeled as belonging to the class C\\
The ROC curve is the curve generated by traveling from T = 0 to T = 1 with each value of the threshold T generating a point (False Positive, True Positive)\cite{Melo2013, Hajian-Tilaki2013}. The different ROC curves based on output may be observed in \autoref{fig:my label-2}. A good model will have a curve that climbs quickly from 0 to 1.. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{Figure/13__ml.png}
    \caption{ROC curves depending on the effectiveness of the model}
    \label{fig:my_label-2}
\end{figure}

Few defining terms:-\\
\textbf{TPR (True Positive Rate) / Recall /Sensitivity)} \\

\begin{equation*}
    TPR (True Positive Rate) / Recall /Sensitivity) = \frac{TP}{TP+FN}
\end{equation*}

\textbf{Specificity}\\
\begin{equation*}
    Specificity = \frac{TN}{TN+FP}

\end{equation*}

\textbf{FPR} \\
\begin{equation*}
    FPR = 1- Specificity
        = \frac{FP}{TN+FP}
\end{equation*}







%     \textbf{(Maths part may be excluded)}



% 1. \url{https://openlearninglibrary.mit.edu/assets/courseware/v1/9c36c444e5df10eef7ce4d052e4a2ed1/asset-v1:MITx+6.036+1T2019+type@asset+block/notes_chapter_Neural_Networks.pdf}

% \url{https://www.mygreatlearning.com/blog/types-of-neural-networks/}
% \url{MIT lecturre from where you have learnt}



\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{figure}{0}
%\baselineskip 24pt
